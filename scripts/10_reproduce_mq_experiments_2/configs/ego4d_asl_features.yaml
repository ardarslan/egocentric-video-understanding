dataset_name: ego4d
train_split: ['train']
val_split: ['val']
init_rand_seed: 0
dataset: {
  json_file: /home/aarslan/mq/scripts/08_reproduce_mq_experiments/data/ego4d/ego4d_clip_annotations_v3.json,
  video_feat_names: ['ego4d_data/v2/egovlp_egonce',
                     'ego4d_data/v2/slowfast_clip',
                     'ego4d_data/v2/omnivore_clip',
                     'ego4d_data/v2/internvideo'],
  frame_feat_names: [],
  file_prefix: ,
  file_ext: .pt,
  num_classes: 110,
  input_dim: [256, 2304, 1536, 1024],
  feat_stride: 1,
  num_frames: 1,
  trunc_thresh: 0.3,
  crop_ratio: [0.9, 1.0],
  max_seq_len: 1024,
  force_upsampling: True,
}
model: {
  backbone_arch: [3, 3, 9],
  regression_range: [[0, 4], [2, 8], [4, 16], [8, 32], [16, 64], [32, 128], [64, 256], [128, 512], [256, 1024], [512, 10000]],
  fpn_type: identity,
  max_buffer_len_factor: 1.0,
  # 1024 -> 512 -> 256  -> 128 -> 64 -> 32 -> 16 -> 8 -> 4 -> 2
  # shrink the model for reduced input feature channels
  n_head: 16,
  embd_dim: [256, 128, 128, 256],
  fpn_dim: 768,
  head_dim: 768,
  use_abs_pe: True,
}
opt: {
  learning_rate: 0.0001,
  epochs: 10,
  weight_decay: 0.05,
}
loader: {
  batch_size: 1,
  num_workers: 4
}
train_cfg: {
  init_loss_norm: 100,
  clip_grad_l2norm: 1.0,
  cls_prior_prob: 0.01,
  center_sample: radius,
  center_sample_radius: 1.5,
  dropout: 0.1,
  droppath: 0.1,
  t_c_alpha: 0.8,
  al_loss_weight: 0.2,
  seg_loss_weight: 0.0,
  cont_loss_weight: 0.0
}

test_cfg: {
  voting_thresh: 0.9,
  pre_nms_topk: 5000,
  max_seg_num: 200,
  min_score: 0.0001,
  multiclass_nms: True,
  nms_sigma: 0.99,
  duration_thresh: 0.01,
}
output_folder: checkpoints
