{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28806, 3) (7203, 3)\n",
      "(13206, 3) (3303, 3)\n",
      "      frame_index                       question  \\\n",
      "1056          528  What does the image describe?   \n",
      "1068          528  What does the image describe?   \n",
      "1080          528  What does the image describe?   \n",
      "1092          528  What does the image describe?   \n",
      "\n",
      "                                              answer  \n",
      "1056  The image describes a car with a broken engine  \n",
      "1068  The image describes a car with a broken engine  \n",
      "1080  The image describes a car with a broken engine  \n",
      "1092  The image describes a car with a broken engine  \n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "How can this be possible?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m         frame_index \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     70\u001b[0m         question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 71\u001b[0m         fill_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_fill_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblip2_vqa_answers_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblip2_vqa_answers_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m         blip2_vqa_answers_df\u001b[38;5;241m.\u001b[39miat[index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fill_value\n\u001b[1;32m     74\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSCRATCH\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mego4d_data/v2/clips\u001b[39m\u001b[38;5;124m\"\u001b[39m, clip_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[30], line 45\u001b[0m, in \u001b[0;36mget_fill_value\u001b[0;34m(blip2_vqa_answers_df, frame_index, question)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blip2_vqa_row) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(blip2_vqa_row)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow can this be possible?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blip2_vqa_row) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m blip2_vqa_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mException\u001b[0m: How can this be possible?"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "annotations_json_file_path = f\"{os.environ['CODE']}/scripts/07_reproduce_baseline_results/data/ego4d/ego4d_clip_annotations_v3.json\"\n",
    "\n",
    "with open(annotations_json_file_path, \"r\") as reader:\n",
    "    annotations = json.load(reader)\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def process_blip2_answers(blip2_answers):\n",
    "    docs = [nlp(blip2_answer) for blip2_answer in blip2_answers]\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        words.extend([token.lemma_.lower() for token in doc if (token.lemma_.isalpha()) and (not token.is_stop) and (token.text != \"no_answer\")])\n",
    "    return words\n",
    "\n",
    "action_category_blip2_answer_word_tfs = dict()\n",
    "blip2_answer_word_dfs = dict()\n",
    "\n",
    "def fill_missing_cells(df_group):\n",
    "    df_group_sorted = df_group.sort_values(by=\"frame_index\").reset_index(drop=False)\n",
    "    delete_first_row = False\n",
    "    for index, row in df_group_sorted.iterrows():\n",
    "        if pd.isnull(row[\"answer\"]):\n",
    "            if index == 0:\n",
    "                delete_first_row = True\n",
    "            else:\n",
    "                df_group_sorted.at[index, \"answer\"] = df_group_sorted.at[index - 1, \"answer\"]\n",
    "    if delete_first_row:\n",
    "        df_group_sorted = df_group_sorted.iloc[1:, :]\n",
    "    return df_group_sorted\n",
    "\n",
    "def get_fill_value(blip2_vqa_answers_df: pd.DataFrame, frame_index: int, question: str):\n",
    "    blip2_vqa_row = blip2_vqa_answers_df[(blip2_vqa_answers_df[\"frame_index\"] == frame_index - 6) & (blip2_vqa_answers_df[\"question\"] == question)]\n",
    "    if len(blip2_vqa_row) > 1:\n",
    "        print(blip2_vqa_row)\n",
    "        raise Exception(\"How can this be possible?\")\n",
    "    if len(blip2_vqa_row) == 1:\n",
    "        return blip2_vqa_row[\"answer\"]\n",
    "    else:\n",
    "        if frame_index == 0:\n",
    "            return \"no_answer\"\n",
    "        else:\n",
    "            return get_fill_value(blip2_vqa_answers_df=blip2_vqa_answers_df, frame_index=frame_index - 6, question=question)\n",
    "\n",
    "\n",
    "\n",
    "for clip_id in os.listdir(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/frame_features\")):\n",
    "    current_annotations = annotations[clip_id][\"annotations\"]\n",
    "    if len(current_annotations) == 0:\n",
    "        continue\n",
    "    if not os.path.exists(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/frame_features/\", clip_id, \"blip2_vqa_features.tsv\")):\n",
    "        continue\n",
    "    blip2_vqa_answers_df = pd.read_csv(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/frame_features/\", clip_id, \"blip2_vqa_features.tsv\"), sep=\"\\t\")\n",
    "    blip2_vqa_answers_df = blip2_vqa_answers_df.reset_index(drop=True)\n",
    "\n",
    "    print(blip2_vqa_answers_df.shape, blip2_vqa_answers_df.drop_duplicates(subset=['frame_index', 'question']).shape)\n",
    "\n",
    "    for index, row in blip2_vqa_answers_df.iterrows():\n",
    "        if pd.isnull(row[\"answer\"]):\n",
    "            frame_index = row[\"frame_index\"]\n",
    "            question = row[\"question\"]\n",
    "            fill_value = get_fill_value(blip2_vqa_answers_df=blip2_vqa_answers_df, frame_index=frame_index, question=question)\n",
    "            blip2_vqa_answers_df.iat[index, \"answer\"] = fill_value\n",
    "\n",
    "    cap = cv2.VideoCapture(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/clips\", clip_id + \".mp4\"))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "    for frame_id in range(0, num_frames, 6):\n",
    "        current_blip2_answers = blip2_vqa_answers_df[blip2_vqa_answers_df[\"frame_index\"] == frame_id]\n",
    "        current_blip2_happen_answer = current_blip2_answers[current_blip2_answers[\"question\"] == \"What is happening in this picture?\"][\"answer\"].values[0]\n",
    "        current_blip2_describe_answer = current_blip2_answers[current_blip2_answers[\"question\"] == \"What does the image describe?\"][\"answer\"].values[0]\n",
    "        current_blip2_do_answer = current_blip2_answers[current_blip2_answers[\"question\"] == \"What is the person in this picture doing?\"][\"answer\"].values[0]\n",
    "        blip2_answers = [current_blip2_happen_answer, current_blip2_describe_answer, current_blip2_do_answer]\n",
    "        blip2_answer_words = process_blip2_answers(blip2_answers)\n",
    "\n",
    "        for current_annotation in current_annotations:\n",
    "            if frame_id / fps >= current_annotation[\"segment\"][0] and frame_id / fps <= current_annotation[\"segment\"][1]:\n",
    "                for blip2_answer_word in blip2_answer_words:\n",
    "                    if current_annotation[\"label\"] not in action_category_blip2_answer_word_tfs.keys():\n",
    "                        action_category_blip2_answer_word_tfs[current_annotation[\"label\"]] = dict()\n",
    "                        action_category_blip2_answer_word_tfs[current_annotation[\"label\"]][blip2_answer_word] = 1\n",
    "                    else:\n",
    "                        if blip2_answer_word not in action_category_blip2_answer_word_tfs[current_annotation[\"label\"]].keys():\n",
    "                            action_category_blip2_answer_word_tfs[current_annotation[\"label\"]][blip2_answer_word] = 1\n",
    "                        else:\n",
    "                            action_category_blip2_answer_word_tfs[current_annotation[\"label\"]][blip2_answer_word] += 1\n",
    "                    \n",
    "                    if blip2_answer_word not in blip2_answer_word_dfs.keys():\n",
    "                        blip2_answer_word_dfs[blip2_answer_word] = 1\n",
    "                    else:\n",
    "                        blip2_answer_word_dfs[blip2_answer_word] += 1\n",
    "\n",
    "blip2_answer_word_idfs = dict()\n",
    "for blip2_answer_word in blip2_answer_word_dfs.keys():\n",
    "    blip2_answer_word_idfs[blip2_answer_word] += math.log(len(blip2_answer_word_dfs.keys()) / (1 + float(blip2_answer_word_dfs[blip2_answer_word])), 2)\n",
    "\n",
    "action_category_blip2_word_tf_idfs = dict()\n",
    "for action_category in action_category_blip2_answer_word_tfs.keys():\n",
    "    action_category_blip2_word_tf_idfs[action_category] = dict()\n",
    "\n",
    "    for blip2_answer_word in action_category_blip2_answer_word_tfs[action_category].keys():\n",
    "        action_category_blip2_word_tf_idfs[action_category][blip2_answer_word] = action_category_blip2_answer_word_tfs[action_category][blip2_answer_word] * blip2_answer_word_idfs[blip2_answer_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'011eb377-92f6-4820-9765-81d822c9a2a5'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('frame_index', 'question')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('frame_index', 'question')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mblip2_vqa_answers_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mframe_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: ('frame_index', 'question')"
     ]
    }
   ],
   "source": [
    "blip2_vqa_answers_df[\"frame_index\", \"question\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8a67af0d90>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.sort(blip2_vqa_answers_df[\"frame_index\"].values), np.sort(blip2_vqa_answers_df[\"frame_index\"].values), marker=\"o\", color=\"red\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "annotations_json_file_path = f\"{os.environ['CODE']}/scripts/07_reproduce_baseline_results/data/ego4d/ego4d_clip_annotations_v3.json\"\n",
    "clip_id = \"003c5ae8-3abd-4824-8efb-21a9a4f8eafe\"\n",
    "\n",
    "with open(annotations_json_file_path, \"r\") as reader:\n",
    "    annotations = json.load(reader)[clip_id][\"annotations\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "annotations_json_file_path = f\"{os.environ['CODE']}/scripts/07_reproduce_baseline_results/data/ego4d/ego4d_clip_annotations_v3.json\"\n",
    "\n",
    "with open(annotations_json_file_path, \"r\") as reader:\n",
    "    annotations = json.load(reader)[clip_id][\"annotations\"]\n",
    "\n",
    "cap = cv2.VideoCapture(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/clips\", \"003c5ae8-3abd-4824-8efb-21a9a4f8eafe.mp4\"))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "num_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "for frame_id in range(num_frames):\n",
    "    for annotations_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dash, html, dcc, Input, Output, no_update\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from dash import Dash, html, dcc, Input, Output, no_update\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils import extract_frames\n",
    "\n",
    "random.seed(1903)\n",
    "\n",
    "ground_truth_asl_predicted_action_category_match_color_mapping = {\n",
    "    True: \"rgba(0, 255, 0, 1.0)\",\n",
    "    False: \"rgba(255, 0, 0, 1.0)\",\n",
    "}\n",
    "\n",
    "unique_action_categories = set([\"background\", \"no_annotations_for_the_clip\"])\n",
    "\n",
    "\n",
    "def generate_random_color():\n",
    "    random_int = np.random.randint(low=0, high=256, size=(3,))\n",
    "    random_color = f\"rgba({random_int[0]}, {random_int[1]}, {random_int[2]}, 1.0)\"\n",
    "    return random_color\n",
    "\n",
    "\n",
    "def get_blip2_answer(current_blip2_rows, blip2_question):\n",
    "    answer = current_blip2_rows[current_blip2_rows[\"question\"] == blip2_question][\n",
    "        \"answer\"\n",
    "    ]\n",
    "    if len(answer) == 0:\n",
    "        return \"NaN\"\n",
    "    else:\n",
    "        return answer.values[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Argument parser\")\n",
    "    parser.add_argument(\n",
    "        \"--clip_id\",\n",
    "        type=str,\n",
    "        default=\"003c5ae8-3abd-4824-8efb-21a9a4f8eafe\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ground_truth_action_instances_file_path\",\n",
    "        type=str,\n",
    "        default=f\"{os.environ['CODE']}/scripts/07_reproduce_baseline_results/data/ego4d/ego4d_clip_annotations_v3.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--asl_predicted_action_instances_file_path\",\n",
    "        type=str,\n",
    "        default=f\"{os.environ['CODE']}/scripts/07_reproduce_baseline_results/submission_final.json\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--assets_path\",\n",
    "        type=str,\n",
    "        default=f\"{os.environ['SCRATCH']}/ego4d_data/v2/frames\",\n",
    "    )\n",
    "    parser.add_argument(\"--frame_feature_extraction_stride\", type=int, default=6)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            os.environ[\"CODE\"],\n",
    "            \"scripts/07_reproduce_baseline_results/data/ego4d/ego4d_clip_annotations_v3.json\",\n",
    "        ),\n",
    "        \"r\",\n",
    "    ) as reader:\n",
    "        annotations_dict = json.load(reader)\n",
    "\n",
    "    if (\n",
    "        not os.path.exists(\n",
    "            os.path.join(\n",
    "                os.environ[\"SCRATCH\"],\n",
    "                \"ego4d_data/v2/frames\",\n",
    "                args.clip_id,\n",
    "                \"end.txt\",\n",
    "            )\n",
    "        )\n",
    "        or not (len(annotations_dict[args.clip_id][\"annotations\"]) > 0)\n",
    "        or not os.path.exists(\n",
    "            os.path.join(\n",
    "                os.environ[\"SCRATCH\"],\n",
    "                \"ego4d_data/v2/frame_features\",\n",
    "                args.clip_id,\n",
    "                \"blip2_vqa_features.tsv\",\n",
    "            )\n",
    "        )\n",
    "    ):\n",
    "        raise Exception(\"Please choose another clip.\")\n",
    "\n",
    "    ground_truth_action_instances = json.load(\n",
    "        open(args.ground_truth_action_instances_file_path, \"r\")\n",
    "    )[args.clip_id][\"annotations\"]\n",
    "    asl_predicted_action_instances = json.load(\n",
    "        open(args.asl_predicted_action_instances_file_path, \"r\")\n",
    "    )[\"detect_results\"][args.clip_id]\n",
    "    blip2_answers_folder_path = os.path.join(\n",
    "        os.environ[\"SCRATCH\"], \"ego4d_data/v2/frame_features\", args.clip_id\n",
    "    )\n",
    "    blip2_answers_file_names = [\n",
    "        file_name\n",
    "        for file_name in os.listdir(blip2_answers_folder_path)\n",
    "        if file_name.startswith(\"blip2_\")\n",
    "    ]\n",
    "    blip2_answers_file_paths = [\n",
    "        os.path.join(\n",
    "            os.environ[\"SCRATCH\"],\n",
    "            \"ego4d_data/v2/frame_features\",\n",
    "            args.clip_id,\n",
    "            blip2_answers_file_name,\n",
    "        )\n",
    "        for blip2_answers_file_name in blip2_answers_file_names\n",
    "    ]\n",
    "    blip2_answers_dfs = pd.concat(\n",
    "        [\n",
    "            pd.read_csv(blip2_answers_file_path, sep=\"\\t\")\n",
    "            for blip2_answers_file_path in blip2_answers_file_paths\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(\n",
    "        os.path.join(\n",
    "            os.environ[\"SCRATCH\"], \"ego4d_data/v2/clips\", args.clip_id + \".mp4\"\n",
    "        )\n",
    "    )\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    extract_frames(clip_id=args.clip_id, output_folder_path=args.assets_path)\n",
    "\n",
    "    frame_id_ground_truth_action_categories_mapping = {}\n",
    "    frame_id_asl_predicted_action_categories_mapping = {}\n",
    "\n",
    "    for current_frame_id in range(num_frames):\n",
    "        frame_id_asl_predicted_action_categories_mapping[current_frame_id] = []\n",
    "        current_frame_time = current_frame_id / fps\n",
    "        assigned_to_an_action_category = False\n",
    "        for asl_predicted_action_instance in asl_predicted_action_instances:\n",
    "            if (\n",
    "                current_frame_time >= asl_predicted_action_instance[\"segment\"][0]\n",
    "                and current_frame_time <= asl_predicted_action_instance[\"segment\"][1]\n",
    "            ):\n",
    "                assigned_to_an_action_category = True\n",
    "                frame_id_asl_predicted_action_categories_mapping[\n",
    "                    current_frame_id\n",
    "                ].append(\n",
    "                    (\n",
    "                        asl_predicted_action_instance[\"label\"],\n",
    "                        asl_predicted_action_instance[\"score\"],\n",
    "                    )\n",
    "                )\n",
    "                unique_action_categories.add(asl_predicted_action_instance[\"label\"])\n",
    "        if assigned_to_an_action_category:\n",
    "            frame_id_asl_predicted_action_categories_mapping[current_frame_id] = sorted(\n",
    "                frame_id_asl_predicted_action_categories_mapping[current_frame_id],\n",
    "                key=lambda x: x[1],\n",
    "            )[-1][0]\n",
    "        else:\n",
    "            frame_id_asl_predicted_action_categories_mapping[\n",
    "                current_frame_id\n",
    "            ] = \"background\"\n",
    "\n",
    "        if len(ground_truth_action_instances) == 0:\n",
    "            frame_id_ground_truth_action_categories_mapping[\n",
    "                current_frame_id\n",
    "            ] = \"no_annotations_for_the_clip\"\n",
    "        else:\n",
    "            assigned_to_an_action_category = False\n",
    "            for ground_truth_action_instance in ground_truth_action_instances:\n",
    "                if (\n",
    "                    current_frame_time >= ground_truth_action_instance[\"segment\"][0]\n",
    "                    and current_frame_time <= ground_truth_action_instance[\"segment\"][1]\n",
    "                ):\n",
    "                    assigned_to_an_action_category = True\n",
    "                    frame_id_ground_truth_action_categories_mapping[\n",
    "                        current_frame_id\n",
    "                    ] = ground_truth_action_instance[\"label\"]\n",
    "                    unique_action_categories.add(ground_truth_action_instance[\"label\"])\n",
    "            if not assigned_to_an_action_category:\n",
    "                frame_id_ground_truth_action_categories_mapping[\n",
    "                    current_frame_id\n",
    "                ] = \"background\"\n",
    "\n",
    "    action_category_color_mapping = dict(\n",
    "        (action_category, generate_random_color())\n",
    "        for action_category in sorted(list(unique_action_categories))\n",
    "    )\n",
    "\n",
    "    sequences_dict = {\n",
    "        \"gt_colors\": [],\n",
    "        \"asl_pred_colors\": [],\n",
    "        \"match_colors\": [],\n",
    "        \"gt_values\": [],\n",
    "        \"asl_pred_values\": [],\n",
    "        \"match_values\": [],\n",
    "        \"frame_ids\": [],\n",
    "        \"blip2_happen_answers\": [],\n",
    "        \"blip2_do_answers\": [],\n",
    "        \"blip2_describe_answers\": [],\n",
    "        \"blip2_captioning_answers\": [],\n",
    "    }\n",
    "\n",
    "    blip2_describe_question = \"What does the image describe?\"\n",
    "    blip2_do_question = \"What is the person in this picture doing?\"\n",
    "    blip2_happen_question = \"What is happening in this picture?\"\n",
    "    blip2_captioning_question = \"Image Caption\"\n",
    "\n",
    "    for frame_id in range(num_frames):\n",
    "        current_blip2_rows = blip2_answers_dfs[\n",
    "            blip2_answers_dfs[\"frame_index\"]\n",
    "            == (frame_id // args.frame_feature_extraction_stride)\n",
    "            * args.frame_feature_extraction_stride\n",
    "        ]\n",
    "        current_blip2_describe_answer = get_blip2_answer(\n",
    "            current_blip2_rows=current_blip2_rows,\n",
    "            blip2_question=blip2_describe_question,\n",
    "        )\n",
    "        current_blip2_do_answer = get_blip2_answer(\n",
    "            current_blip2_rows=current_blip2_rows, blip2_question=blip2_do_question\n",
    "        )\n",
    "        current_blip2_happen_answer = get_blip2_answer(\n",
    "            current_blip2_rows=current_blip2_rows,\n",
    "            blip2_question=blip2_happen_question,\n",
    "        )\n",
    "        current_blip2_captioning_answer = get_blip2_answer(\n",
    "            current_blip2_rows=current_blip2_rows,\n",
    "            blip2_question=blip2_captioning_question,\n",
    "        )\n",
    "\n",
    "        current_ground_truth_action_category = (\n",
    "            frame_id_ground_truth_action_categories_mapping[frame_id]\n",
    "        )\n",
    "        sequences_dict[\"frame_ids\"].append(frame_id)\n",
    "        sequences_dict[\"gt_values\"].append(current_ground_truth_action_category)\n",
    "        current_ground_truth_action_category_color = action_category_color_mapping[\n",
    "            current_ground_truth_action_category\n",
    "        ]\n",
    "        sequences_dict[\"gt_colors\"].append(current_ground_truth_action_category_color)\n",
    "        sequences_dict[\"blip2_happen_answers\"].append(current_blip2_happen_answer)\n",
    "        sequences_dict[\"blip2_do_answers\"].append(current_blip2_do_answer)\n",
    "        sequences_dict[\"blip2_describe_answers\"].append(current_blip2_describe_answer)\n",
    "        sequences_dict[\"blip2_captioning_answers\"].append(\n",
    "            current_blip2_captioning_answer\n",
    "        )\n",
    "\n",
    "        current_asl_predicted_action_category = (\n",
    "            frame_id_asl_predicted_action_categories_mapping[frame_id]\n",
    "        )\n",
    "        current_asl_predicted_action_category_color = action_category_color_mapping[\n",
    "            current_asl_predicted_action_category\n",
    "        ]\n",
    "        sequences_dict[\"asl_pred_values\"].append(current_asl_predicted_action_category)\n",
    "        sequences_dict[\"asl_pred_colors\"].append(\n",
    "            current_asl_predicted_action_category_color\n",
    "        )\n",
    "\n",
    "        current_ground_truth_asl_predicted_action_category_match = (\n",
    "            current_ground_truth_action_category\n",
    "            == current_asl_predicted_action_category\n",
    "        )\n",
    "        current_ground_truth_asl_predicted_action_category_match_color = (\n",
    "            ground_truth_asl_predicted_action_category_match_color_mapping[\n",
    "                current_ground_truth_asl_predicted_action_category_match\n",
    "            ]\n",
    "        )\n",
    "        sequences_dict[\"match_values\"].append(\n",
    "            current_ground_truth_asl_predicted_action_category_match\n",
    "        )\n",
    "        sequences_dict[\"match_colors\"].append(\n",
    "            current_ground_truth_asl_predicted_action_category_match_color\n",
    "        )\n",
    "\n",
    "    sequences_dict[\"frame_file_paths\"] = [\n",
    "        os.path.join(\n",
    "            args.clip_id,\n",
    "            frame_file_name,\n",
    "        )\n",
    "        for frame_file_name in os.listdir(\n",
    "            os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/frames\", args.clip_id)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    fig = go.Figure(\n",
    "        data=[\n",
    "            go.Bar(\n",
    "                orientation=\"h\",\n",
    "                x=[1] * num_frames,\n",
    "                y=[name] * num_frames,\n",
    "                marker=dict(\n",
    "                    color=sequences_dict[f\"{name}_colors\"],\n",
    "                    line=dict(color=\"rgb(255, 255, 255)\", width=0),\n",
    "                ),\n",
    "                customdata=list(\n",
    "                    zip(\n",
    "                        sequences_dict[\"frame_file_paths\"],\n",
    "                        sequences_dict[\"frame_ids\"],\n",
    "                        sequences_dict[\"gt_values\"],\n",
    "                        sequences_dict[\"asl_pred_values\"],\n",
    "                        sequences_dict[\"match_values\"],\n",
    "                        sequences_dict[\"blip2_describe_answers\"],\n",
    "                        sequences_dict[\"blip2_do_answers\"],\n",
    "                        sequences_dict[\"blip2_happen_answers\"],\n",
    "                        sequences_dict[\"blip2_captioning_answers\"],\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            for name in [\"match\", \"asl_pred\", \"gt\"]\n",
    "        ],\n",
    "        layout=dict(\n",
    "            title=f\"Clip ID: {args.clip_id}\",\n",
    "            barmode=\"stack\",\n",
    "            barnorm=\"fraction\",\n",
    "            bargap=0.5,\n",
    "            showlegend=False,\n",
    "            xaxis=dict(range=[-0.02, 1.02], showticklabels=False, showgrid=False),\n",
    "            height=max(600, 40 * len(sequences_dict.keys())),\n",
    "            template=None,\n",
    "            margin=dict(b=1),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_traces(hoverinfo=\"none\", hovertemplate=None)\n",
    "\n",
    "    app = Dash(__name__, assets_folder=args.assets_path)\n",
    "\n",
    "    app.layout = html.Div(\n",
    "        [\n",
    "            dcc.Graph(id=\"graph-basic-2\", figure=fig, clear_on_unhover=True),\n",
    "            dcc.Tooltip(id=\"graph-tooltip\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    @app.callback(\n",
    "        Output(\"graph-tooltip\", \"show\"),\n",
    "        Output(\"graph-tooltip\", \"bbox\"),\n",
    "        Output(\"graph-tooltip\", \"children\"),\n",
    "        Input(\"graph-basic-2\", \"hoverData\"),\n",
    "    )\n",
    "    def display_hover(hoverData):\n",
    "        if hoverData is None:\n",
    "            return False, no_update, no_update\n",
    "\n",
    "        bbox = hoverData[\"points\"][0][\"bbox\"]\n",
    "\n",
    "        children = [\n",
    "            html.Div(\n",
    "                [\n",
    "                    html.Img(\n",
    "                        src=app.get_asset_url(hoverData[\"points\"][0][\"customdata\"][0]),\n",
    "                        style={\"width\": \"100%\"},\n",
    "                    ),\n",
    "                    html.P(f\"Frame ID: {hoverData['points'][0]['customdata'][1]}\"),\n",
    "                    html.P(\n",
    "                        f\"Ground Truth: {str(hoverData['points'][0]['customdata'][2]).replace('_', ' ')}\"\n",
    "                    ),\n",
    "                    html.P(\n",
    "                        f\"ASL Prediction: {str(hoverData['points'][0]['customdata'][3]).replace('_', ' ')}\"\n",
    "                    ),\n",
    "                    html.P(\n",
    "                        f\"Match: {str(hoverData['points'][0]['customdata'][4]).replace('_', ' ')}\"\n",
    "                    ),\n",
    "                    html.P(\n",
    "                        f\"What does the image describe? (BLIP2): {hoverData['points'][0]['customdata'][5]}\"\n",
    "                    ),\n",
    "                    html.P(\n",
    "                        f\"What is the person in this picture doing? (BLIP2): {hoverData['points'][0]['customdata'][6]}\"\n",
    "                    ),\n",
    "                    html.P(\n",
    "                        f\"What is happening in this picture? (BLIP2): {hoverData['points'][0]['customdata'][7]}\"\n",
    "                    ),\n",
    "                    html.P(\n",
    "                        f\"Image Caption (BLIP2): {hoverData['points'][0]['customdata'][8]}\"\n",
    "                    ),\n",
    "                ],\n",
    "                style={\"width\": \"400px\", \"white-space\": \"normal\"},\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return True, bbox, children\n",
    "\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('mq_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7baf97c684abb9a3f3dba5fbfca0f2f069a7ab5cf394abaf3794cf37d30c0a2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
