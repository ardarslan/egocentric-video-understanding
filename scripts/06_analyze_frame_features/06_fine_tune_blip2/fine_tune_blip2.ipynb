{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/torch/__init__.py:1256\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m __config__ \u001b[39mas\u001b[39;00m __config__\n\u001b[1;32m   1255\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m __future__ \u001b[39mas\u001b[39;00m __future__\n\u001b[0;32m-> 1256\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m profiler \u001b[39mas\u001b[39;00m profiler\n\u001b[1;32m   1258\u001b[0m \u001b[39m# Quantized, sparse, AO, etc. should be last to get imported, as nothing\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[39m# is expected to depend on them.\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m ao \u001b[39mas\u001b[39;00m ao\n",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/torch/profiler/__init__.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprofiler\u001b[39;00m \u001b[39mimport\u001b[39;00m record_function, KinetoStepTracker\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m register_optimizer_step_post_hook\n\u001b[0;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mprofiler\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     _KinetoProfile,\n\u001b[1;32m     19\u001b[0m     ExecutionGraphObserver,\n\u001b[1;32m     20\u001b[0m     profile,\n\u001b[1;32m     21\u001b[0m     ProfilerAction,\n\u001b[1;32m     22\u001b[0m     schedule,\n\u001b[1;32m     23\u001b[0m     supported_activities,\n\u001b[1;32m     24\u001b[0m     tensorboard_trace_handler,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mprofile\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mschedule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExecutionGraphObserver\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m itt\n",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/torch/profiler/profiler.py:20\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_profiler\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     13\u001b[0m     _add_execution_graph_observer,\n\u001b[1;32m     14\u001b[0m     _disable_execution_graph_observer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     _remove_execution_graph_observer,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m \u001b[39mimport\u001b[39;00m kineto_available, ProfilerActivity\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprofiler\u001b[39;00m \u001b[39mimport\u001b[39;00m _memory_profiler\n\u001b[1;32m     23\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39msupported_activities\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mProfilerAction\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExecutionGraphObserver\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m ]\n\u001b[1;32m     31\u001b[0m PROFILER_STEP_NAME \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mProfilerStep\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/torch/profiler/_memory_profiler.py:31\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_profiler\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     _EventType,\n\u001b[1;32m     24\u001b[0m     _ExtraFields_Allocation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     RecordScope,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m _element_size\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprofiler\u001b[39;00m \u001b[39mimport\u001b[39;00m _utils\n\u001b[1;32m     33\u001b[0m TensorAndID \u001b[39m=\u001b[39m Tuple[\u001b[39m\"\u001b[39m\u001b[39mTensorKey\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mint\u001b[39m]\n\u001b[1;32m     35\u001b[0m log \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1040\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import ray\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "sys.path.insert(\n",
    "    0,\n",
    "    os.path.join(\n",
    "        os.environ[\"CODE\"],\n",
    "        \"scripts/04_extract_frame_features/\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from constants import question_constant_mapping\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import Blip2Processor\n",
    "from transformers.models.blip_2.configuration_blip_2 import Blip2Config\n",
    "from transformers.models.auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from transformers.models.blip_2.modeling_blip_2 import Blip2PreTrainedModel, Blip2VisionModel, Blip2QFormerModel, Blip2ForConditionalGenerationModelOutput\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "seed = 1903\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/analysis_data/ground_truth_labels/ground_truth_labels.pickle\"), \"rb\") as reader:\n",
    "    ground_truth_label_indices = pickle.load(reader)\n",
    "    clip_ids = shuffle(list(ground_truth_label_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(clip_ids) * 0.6)\n",
    "val_size = int(len(clip_ids) * 0.2)\n",
    "test_size = int(len(clip_ids) * 0.2)\n",
    "\n",
    "train_clip_ids = clip_ids[:train_size]\n",
    "val_clip_ids = clip_ids[train_size:train_size+val_size]\n",
    "test_clip_ids = clip_ids[train_size+val_size:]\n",
    "\n",
    "def get_random_frame_and_corresponding_label():\n",
    "    random_clip_id_index = np.random.randint(low=0, high=len(clip_ids))\n",
    "    random_clip_id = clip_ids[random_clip_id_index]\n",
    "    random_cap = cv2.VideoCapture(os.path.join(os.environ[\"SCRATCH\"], \"ego4d_data/v2/clips\", random_clip_id + \".mp4\"))\n",
    "    number_of_frames = random_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    random_frame_id = np.random.randint(low=0, high=number_of_frames)\n",
    "    random_cap.set(cv2.CAP_PROP_POS_FRAMES, random_frame_id - 1)\n",
    "    success, random_frame = random_cap.read()\n",
    "    corresponding_label = ground_truth_label_indices[random_clip_id][random_frame_id]\n",
    "    return random_frame, corresponding_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:29<00:00, 44.54s/it]\n"
     ]
    }
   ],
   "source": [
    "blip2_vqa_frame_feature_extractor = BLIP2VQAFrameFeatureExtractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/aarslan/mambaforge/envs/mq_analysis/lib/python3.9/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    random_frame, corresponding_label = get_random_frame_and_corresponding_label()\n",
    "    preprocessed_frames_batch_dict = blip2_vqa_frame_feature_extractor.processor(\n",
    "        images=[Image.fromarray(random_frame[:, :, ::-1])],\n",
    "        text=[\"Question: \" + \"What is the person in this image doing?\" + \" Answer:\"],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda:3\", torch.float16)\n",
    "    generated_ids = blip2_vqa_frame_feature_extractor.model.generate(**preprocessed_frames_batch_dict)\n",
    "    # blip2_answer = blip2_vqa_frame_feature_extractor.processor.batch_decode(\n",
    "    #     generated_ids, skip_special_tokens=True\n",
    "    # )[0].strip()\n",
    "    # print(blip2_answer)\n",
    "    # display(Image.fromarray(random_frame[:, :, ::-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.environ[\"CODE\"], \"scripts/06_analyze_frame_features/02_map_label_dependency_parsing_features_and_blip2_answer_dependency_parsing_features\", \"label_verb_noun_tool_mapping.json\"), \"r\") as reader:\n",
    "    label_verb_noun_tools_mapping = json.load(reader)\n",
    "\n",
    "label_verb_noun_tools_mapping_keys = sorted(list(label_verb_noun_tools_mapping.keys())) + [\"background\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('mq_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e577bcfaded8c1034b24fd1c2fc1f0e8dde78215fba17c67cdf3d691832ce790"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
